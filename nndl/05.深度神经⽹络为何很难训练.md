## 深度神经⽹络为何很难训练

上一章提到了神经网络的一种普遍性，比如说不管目标函数是怎样的，神经网络总是能够对任何可能的输入得到一个近似的输出。

> 普遍性告诉我们神经⽹络能计算任何函数；而实际经验依据提⽰深度⽹络最能适⽤于学习能够解决许多现实世界问题的函数

而且理论上我们只要一个隐藏层就可以计算任何函数，第一章我们就用如下的网络结构完成了一个手写字识别的模型：

![image-20201119214307565](https://raw.githubusercontent.com/howie6879/oss/master/uPic/image-20201119214307565.png)

这时候，大家心中可能都会有这样一个想法，如果加大网络的深度，模型的识别准确率是否会提高？

![image-20201119214431316](https://raw.githubusercontent.com/howie6879/oss/master/uPic/image-20201119214431316.png)

随即我们会基于反向传播的随即梯度下降来训练神经网络，但实际上这会产生一些问题，因为我们的深度神经网络并没有比浅层网络好很多。那么此处就引出了一个问题，为什么深度神经网络相对训练困难？

仔细研究会发现：

- 如果网络后面层学习状况好的时候，前面的层次经常会在训练时停滞不前

- 反之情况也会发生，前面层训练良好，后面停滞不前

实际上，我们发现在深度神经⽹络中使⽤基于梯度下降的学习⽅法本⾝存在着内在不稳定性，这种不稳定性使得先前或者后⾯神经网络层的学习过程阻滞。

### 消失的梯度问题

